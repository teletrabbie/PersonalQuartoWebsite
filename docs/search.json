[
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Quarto-Projekte",
    "section": "",
    "text": "Hier findet ihr die seit 2024 erstellten Quarto-Projekte. Bei dieser Quarto-Seite handelt es sich um ein “listing”, das auch für einen Quarto-Blog verwendet wird. Die einzelnen Blog-Einträge sind separate qmd-Files, die automatisch in diese Liste hinzugefügt werden. Ausserdem werden R Markdown (.Rmd) oder Jupyter Notebooks (.ipynb) problemlos eingebunden.\nWeitere Infos findet Ihr auf quarto.org.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNodes 2025\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2025\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nMedikamenten-Graph\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2025\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nZusammenfassung ‘Introduction to Deep Learning’\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nRSA und Modulo-exponentiation\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2025\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Präsentationen\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nMirth starten nach dem Booten\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Mirth Connect (Step-by-step)\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping mit Python\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nRestAPI mit R\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nInteroperabilität\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nEinfach Schriftsteller werden\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nPubmed mit R abfragen\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nR Markdown in Quarto-Seite\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nJupiter Notebooks und Quarto\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nVon .Rmd zu .qmd\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nChristian Franke\n\n\n\n\n\n\n\n\n\n\n\n\nEs kann losgehen\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nChristian Franke\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html",
    "href": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html",
    "title": "Von .Rmd zu .qmd",
    "section": "",
    "text": "Ist es einfach, eine R Markdown Datei als Quarto Datei zu ersetzen? Man sagt: ja. Und ich habe es ausprobiert.\nIch habe ein altes, kleines Statistik-Projekt aus dem zweiten Semester genutzt und den R Markdown Code in die Quarto Datei hineinkopiert. Das Ergebnis war sehr gut! Nur zwei Unterschiede sind dabei aufgefallen:\nEin Unterschied zwischen R Markdown und Quarto sind die nicht vorhandenen Chunk-Optionen bei Quarto. Diese fande ich noch praktisch, aber gut, nun muss ich sie mir endlich mal merken."
  },
  {
    "objectID": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#umfrage",
    "href": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#umfrage",
    "title": "Von .Rmd zu .qmd",
    "section": "Umfrage",
    "text": "Umfrage\n\nDie Aufgabenstellung “Männer essen Fleisch öfter als Frauen” haben wir präzisiert, um die Datenqualität zu erhöhen.\nFrage: “An wie vielen Tagen pro Woche essen Sie Fleisch?”\nUmfrage in Bern Bümpliz vor einem Coop am 23.02.2022.\nUmfrage in Bern Länggasse vor einer Migros am 26.02.2022.\nGanzzahlige Antworten zwischen 0 bis 7 waren möglich.\nEs wurden das Geschlecht (m/w) sowie die Altersgruppe erfasst.\nEs wurden insgesamt 75 Datenpunkte gesammelt.\nAufgrund der Umfrageorte und Umfragezeiten repräsentieren die Daten nur zum Teil die gesamte Bevölkerung (Details siehe im Ausblick).\nZusammenfassung der Datensätze in einer gemeinsamen csv-Datei und Analyse in R (Markdown).\n\n\ndata &lt;- read.csv(\"input_data_fleisch.csv\", sep = \";\")\nsummary(data)\n\n AnzTageFleisch  Geschlecht        Altersgruppe      \n Min.   :0.00   Length:75          Length:75         \n 1st Qu.:1.00   Class :character   Class :character  \n Median :3.00   Mode  :character   Mode  :character  \n Mean   :2.84                                        \n 3rd Qu.:4.00                                        \n Max.   :7.00                                        \n\nm &lt;- data[data$Geschlecht==\"m\", c(\"AnzTageFleisch\")]   #length(m); mean(m); sd(m)\nw &lt;- data[data$Geschlecht==\"w\", c(\"AnzTageFleisch\")]   #length(w); mean(w); sd(w)"
  },
  {
    "objectID": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#deskriptive-datenanalyse",
    "href": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#deskriptive-datenanalyse",
    "title": "Von .Rmd zu .qmd",
    "section": "Deskriptive Datenanalyse",
    "text": "Deskriptive Datenanalyse\nFür Männer (n=43) liegt der Mittelwert bei 3.67 und die Standardabweichung bei 1.95.\nFür Frauen (n=32) liegt der Mittelwert bei 1.72 und die Standardabweichung bei 1.82.\n\nboxplot(AnzTageFleisch ~ Geschlecht, data = data\n        , main=\"Anzahl Tage mit Fleischkonsum pro Woche je Geschlecht\")\n\n\n\n\n\n\n\n\nDie Verteilung der Frauen ist eher tiefer im Vergleich zu den Männern. Dies ist ein Hinweis, dass Männer öfters Fleisch essen, aber noch kein “Beweis”. Es müssen zunächst statistische Tests durchgeführt werden."
  },
  {
    "objectID": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#hypothesen",
    "href": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#hypothesen",
    "title": "Von .Rmd zu .qmd",
    "section": "Hypothesen",
    "text": "Hypothesen\nAls Mittelwert verstehen wir hier das arithmetische Mittel aus der “Anzahl Tage mit Fleischkonsum pro Woche” (AnzTageFleisch). Die Nullhypothese lautet: der Mittelwert der Männer ist gleich gross wie der Mittelwert der Frauen. Als alternative Hypothese können wir schreiben: der Mittelwert der Männer ist grösser als der Mittelwert Frauen. Formal:\n\n\\(H_{0}: \\mu_{m} = \\mu_{w}\\)\n\\(H_{1}: \\mu_{m} &gt; \\mu_{w}\\)"
  },
  {
    "objectID": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#methoden",
    "href": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#methoden",
    "title": "Von .Rmd zu .qmd",
    "section": "Methoden",
    "text": "Methoden\nBei den Stichproben für Männer und Frauen handelt es sich um ungepaarte Stichproben, da die beiden Gruppen unabhängig voneinander sind. Weil deutlich mehr als 30 Datenpunkte vorliegen, könnten wir einen z-Test durchführen. Allerdings kennen wir die nicht die exakten Varianzen, sodass wir den t-Test verwenden. Da sich die Standardabweichungen von Frauen und Männer in unserer Umfrage nicht zu sehr unterscheiden, dürfen wir annehmen, dass die unbekannten Varianzen etwa gleich gross sind. Der t-Test ist eine sehr gute Approximation. Da n&gt;30, müssen wir nicht die Voraussetzung auf Normalverteilung prüfen. Das Signifikanzniveau unseres t-Tests beträgt 5%."
  },
  {
    "objectID": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#t-test",
    "href": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#t-test",
    "title": "Von .Rmd zu .qmd",
    "section": "t-Test",
    "text": "t-Test\nWir führen einen einseitigen t-Test in R durch.\n\nt.test(x = m,\n       y = w,\n       alternative = c(\"greater\"),\n       paired = FALSE,\n       conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  m and w\nt = 4.4681, df = 69.281, p-value = 1.499e-05\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 1.225962      Inf\nsample estimates:\nmean of x mean of y \n 3.674419  1.718750 \n\n\nDie Teststatistik ist mit 4.47 sehr gross. Der p-Wert liegt deutlich unter dem Signifikanzniveau von 5% und sogar deutlich unter 1%. Die Nullhypothese kann mit diesem kleinen p-Wert verworfen werden. Die alternative Hypothese wird nicht verworfen. Der Mittelwert der Männer scheint somit signifikant grösser zu sein als der Mittelwert der Frauen, d.h. Männer essen offenbar häufiger Fleisch als Frauen."
  },
  {
    "objectID": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#t-test-ohne-vegetarier-ausreisser-nach-unten",
    "href": "projects/Von Rmd zu qmd/Von Rmd zu qmd.html#t-test-ohne-vegetarier-ausreisser-nach-unten",
    "title": "Von .Rmd zu .qmd",
    "section": "t-Test ohne Vegetarier (Ausreisser nach unten)",
    "text": "t-Test ohne Vegetarier (Ausreisser nach unten)\nProblematisch ist möglicherweise die rechtsschiefe Verteilung bei den Frauen. In der Umfrage hatten 11 Frauen angegeben, dass sie vegetarisch sind und somit an 0 Tagen je Woche Fleisch essen (im Vergleich zu nur 4 vegetarischen Männern). Diese Frauen könnten einen grossen Einfluss auf das Ergebnis haben (ca. ein Drittel der weiblichen Beobachtungspunkte). Deshalb haben wir zusätzlich die Vegetarier aus dem Datensatz ausgeschlossen und den t-Test wiederholt.\n\nm0 &lt;- m[m&gt;0]\nw0 &lt;- w[w&gt;0]\n\nt.test(x = m0,\n       y = w0,\n       alternative = c(\"greater\"),\n       paired = FALSE,\n       conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  m0 and w0\nt = 3.2561, df = 40.943, p-value = 0.001136\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.6919657       Inf\nsample estimates:\nmean of x mean of y \n 4.051282  2.619048 \n\n\nEs fliessen nur noch 21 Frauen und 39 Männer in den Test ohne Ausreisser ein. Dies ist gross genug, um die gleiche Test-Methode durchzuführen. Die Mittelwerte sind erwartungsgemäss höher, aber der p-Wert liegt mit 0.1% noch immer deutlich unter unserem Signifikanznieau. Auch hier wird die Nullhypothese verworfen. Falls wir “alle Ausreisser” (AnzTageFleisch=7) entfernen, wäre der p-Wert weiterhin unter 1%."
  },
  {
    "objectID": "projects/R Markdown direkt einbinden.html",
    "href": "projects/R Markdown direkt einbinden.html",
    "title": "R Markdown in Quarto-Seite",
    "section": "",
    "text": "Vor zwei Tagen habe ich eine alte R Markdown Datei manuell als Quarto-File gespeichert/umgewandelt. Das wäre garnicht nöitig gewesen, denn R Markdown wird bei der Quarto-Website als “listing” direkt eingebunden."
  },
  {
    "objectID": "projects/R Markdown direkt einbinden.html#r-markdown",
    "href": "projects/R Markdown direkt einbinden.html#r-markdown",
    "title": "R Markdown in Quarto-Seite",
    "section": "R Markdown",
    "text": "R Markdown\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "projects/R Markdown direkt einbinden.html#including-plots",
    "href": "projects/R Markdown direkt einbinden.html#including-plots",
    "title": "R Markdown in Quarto-Seite",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "projects/query pubmed data/query pubmed data.html",
    "href": "projects/query pubmed data/query pubmed data.html",
    "title": "Pubmed mit R abfragen",
    "section": "",
    "text": "Im Rahmen einer Seminararbeit wurden eine Literaturrecherche auf Pubmed durchgeführt. Unterschiedliche Search-String-Kombinationen wurden mit R automatisch ausgeführt. Dadurch haben wir ein Mengengerüst erhalten, um die Bedeutung von Subthemen besser einzuschätzen und unsere eigene Such-Strategie zu optimieren.\nDas Poster zu unserer Seminararbeit findet ihr hier.\nDas R-Skript hatte ich erstellt, als ich noch keine Erfahrung mit API hatte. Heute würde ich das Skript vermutlich anders (eleganter) gestalten und in den nächsten Wochen sicherlich auch als verbesserte Variante ausprobieren."
  },
  {
    "objectID": "projects/query pubmed data/query pubmed data.html#vorbereitung",
    "href": "projects/query pubmed data/query pubmed data.html#vorbereitung",
    "title": "Pubmed mit R abfragen",
    "section": "Vorbereitung",
    "text": "Vorbereitung\nAls erstes wurde die API-Adresse als String nachgebildet mit jeweils zwei Begriffen, die im Titel oder im Abstract auf Pubmed gesucht wurden.\n\nlibrary(xml2)\n\nWarning: Paket 'xml2' wurde unter R Version 4.3.2 erstellt\n\nlibrary(tidyr)\n\nWarning: Paket 'tidyr' wurde unter R Version 4.3.2 erstellt\n\nlibrary(knitr)\n\nWarning: Paket 'knitr' wurde unter R Version 4.3.2 erstellt\n\n# set static values of the url\nurl_part_1 &lt;- \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=(\"\nurl_part_3 &lt;- \"[Title/Abstract])AND(\"\nurl_part_5 &lt;- \"[Title/Abstract])\"\n\n# empty vector to store the Count-values\nresult_vector &lt;- integer()\nsearch_string_combination_1 &lt;- character()\nsearch_string_combination_2 &lt;- character()\n\n# search-strings\nsearch_string_1 &lt;- c(\n  \"COPD\", \"asthma\", \"diabetes\", \"heart\", \"hypertension\", \"ophthalmology\"\n  #, \"surgery\", \"covid\", \"rehabilitation\", \"chronic\", \"acute\", \"icu\"\n)\nsearch_string_2 &lt;- c(\"telemedicine\",\"telehealth\",\"telemonitoring\", \"remote\")\n\nBis hierhin wurde noch keine Pubmed-Abfrage durchgeführt."
  },
  {
    "objectID": "projects/query pubmed data/query pubmed data.html#abfrage-auf-pubmed",
    "href": "projects/query pubmed data/query pubmed data.html#abfrage-auf-pubmed",
    "title": "Pubmed mit R abfragen",
    "section": "Abfrage auf Pubmed",
    "text": "Abfrage auf Pubmed\nIn einem doppelten Loop wurden die Such-String-Kombinationen mit paste0() zusammengesetzt, die URL aufgerufen und die Anzahl Treffer (“Count”) extrahiert.\n\nfor (i in 1:length(search_string_1)) {\n  for (j in 1:length(search_string_2)) {\n    # define search string\n    url &lt;- paste0(url_part_1, search_string_1[i], url_part_3, search_string_2[j], url_part_5)\n    pubmed_count &lt;- xml_integer(xml_find_all(read_xml(url), \".//Count\"))\n    # safe count values\n    result_vector &lt;- c(result_vector, pubmed_count)\n    search_string_combination_1 &lt;- c(search_string_combination_1, search_string_1[i])\n    search_string_combination_2 &lt;- c(search_string_combination_2, search_string_2[j])\n    }\n  }\n\nSchliesslich kann das Gesamtergebnis als Tabelle ausgegeben werden.\n\nresult_set &lt;- tibble(search_string_combination_1, search_string_combination_2, result_vector) %&gt;%\n  pivot_wider(names_from = \"search_string_combination_2\", values_from = \"result_vector\")\ncolnames(result_set)[1] &lt;- \"Search string combination\"\nkable(result_set)\n\n\n\n\n\n\n\n\n\n\n\nSearch string combination\ntelemedicine\ntelehealth\ntelemonitoring\nremote\n\n\n\n\nCOPD\n255\n253\n194\n284\n\n\nasthma\n261\n133\n65\n379\n\n\ndiabetes\n1750\n977\n286\n2235\n\n\nheart\n1386\n669\n874\n5569\n\n\nhypertension\n601\n346\n297\n1260\n\n\nophthalmology\n525\n119\n4\n263\n\n\n\n\n\nDer doppelte Loop und die ursprünglichen Pubmed-Abfragen führten zu 205 unterschiedlichen Such-String-Kombinationen. Bis die Daten geladen wurden, dauert es knapp 2 Minuten. Das tönt zwar lange, aber geht schneller, als die Suchen manuell durchzuführen. Im Beispiel oben habe ich die Such-String-Kombinationen nun reduziert, sodass das Deployen schneller vonstattengeht.\nDennoch sehe ich noch Verbesserungspotential in meinem Skript. Das nehme ich mir für die nächste Zeit vor…"
  },
  {
    "objectID": "projects/MIT Intro to DL/mit intro.html",
    "href": "projects/MIT Intro to DL/mit intro.html",
    "title": "Zusammenfassung ‘Introduction to Deep Learning’",
    "section": "",
    "text": "Künstliche Intelligenz ist derzeit überall. Ich habe Googles NotebookLM gebeten, die Youtube-Aufzeichnung des Kurses ‘MIT Introduction to Deep Learning | 6.S191’ vom Januar 2025 zusammenzufassen mit 5 Minuten Lesezeit. Es ist erstaunlich, wie gut die Inhalte erfasst und beschrieben wurden. Aber lest selbst…"
  },
  {
    "objectID": "projects/MIT Intro to DL/mit intro.html#ab-hier-folgt-der-ki-generierte-text",
    "href": "projects/MIT Intro to DL/mit intro.html#ab-hier-folgt-der-ki-generierte-text",
    "title": "Zusammenfassung ‘Introduction to Deep Learning’",
    "section": "Ab hier folgt der KI-generierte Text",
    "text": "Ab hier folgt der KI-generierte Text\nGerne erstelle ich eine Zusammenfassung des Vorlesungsausschnitts, die Sie in etwa 5 Minuten lesen können.\n\nEinleitung und Überblick über Deep Learning\nDie Vorlesung “MIT Introduction to Deep Learning” (6.S191) ist ein einwöchiger Intensivkurs über alle Aspekte des Deep Learning. Die Dozenten Alexander Amini und Ava heißen die Teilnehmer willkommen und betonen die rasante Entwicklung des Feldes. Einleitend wird der Fortschritt der Gesichtserzeugung mittels Deep Learning in den letzten zehn Jahren anhand von Beispielen demonstriert, von einfachen Ergebnissen vor einem Jahrzehnt zu fotorealistischen und dynamischen Videos. Ein 2020 in der Klasse erstelltes Video, das eine geklonte Stimme und ein Gesicht zeigte, benötigte erhebliche Ressourcen (155.000 US-Dollar Compute für ein zweiminütiges Video) und war statisch. 2025 wird eine Live-Demonstration gezeigt, bei der eine Stimme in Echtzeit geklont und für eine dynamische Konversation genutzt wird, was die enormen Fortschritte verdeutlicht. Der Kurs zielt darauf ab, die fundamentalen Techniken zu vermitteln, die diesen Fortschritt ermöglichen, indem Computer lernen, Aufgaben direkt aus Beobachtungen und Daten zu lösen. Der Kurs besteht aus technischen Vorlesungen und praktischen Software-Labs mit TensorFlow und PyTorch, einschließlich Wettbewerben mit Preisen und Gastvorträgen von Industrieexperten.\n\n\nGrundlegende Konzepte\nIntelligenz wird als die Fähigkeit definiert, Informationen zu verarbeiten, um zukünftige Entscheidungen zu treffen. Künstliche Intelligenz (KI) ist die Praxis, künstliche Algorithmen zu entwickeln, um diesen Prozess nachzubilden. Maschinelles Lernen ist ein Teilbereich der KI, der sich darauf konzentriert, Muster in Daten zu lernen, um Entscheidungen zu treffen, ohne explizit programmiert zu werden. Deep Learning ist ein Teilbereich des maschinellen Lernens, der diesen Prozess mithilfe von tiefen neuronalen Netzen durchführt.\n\n\nNeuronale Netze: Das Perzeptron\nDer grundlegende Baustein eines neuronalen Netzes ist das Perzeptron oder Neuron. Es nimmt Eingaben auf, multipliziert diese mit entsprechenden Gewichten, addiert einen Bias und wendet dann eine nichtlineare Aktivierungsfunktion an, um eine Ausgabe zu erzeugen. Die Aktivierungsfunktion, wie die Sigmoid-Funktion (Ausgabe zwischen 0 und 1, nützlich für Wahrscheinlichkeiten) oder die Rectified Linear Unit (ReLU) (Ausgabe ist das Maximum von 0 und der Eingabe), ist entscheidend, um Nichtlinearitäten in das Modell einzuführen. Ohne Nichtlinearitäten wäre das Modell linear und könnte keine komplexen realen Daten abbilden.\n\n\nNetzwerkarchitektur\nEin neuronales Netzwerk besteht aus mehreren miteinander verbundenen Perzeptronen, die in Schichten organisiert sind. Eine dichte Schicht (Dense Layer) ist eine Schicht, in der jede Eingabe mit jeder Ausgabe verbunden ist. Tiefe neuronale Netze bestehen aus dem sequenziellen Stapeln mehrerer linearer Schichten gefolgt von Nichtlinearitäten in einer hierarchischen Weise.\n\n\nTraining Neuronaler Netze\nDas Ziel des Trainings ist es, die Gewichte des Netzwerks so anzupassen, dass der Verlust (loss), ein Maß für die Diskrepanz zwischen den Vorhersagen des Modells und den tatsächlichen Werten (Ground Truth), minimiert wird. Die Verlustfunktion quantifiziert diesen Fehler; Beispiele sind die Softmax-Kreuzentropie-Verlustfunktion für binäre Klassifikation und der mittlere quadratische Fehler (MSE) für Regression.\nDer Prozess zur Minimierung des Verlusts wird Gradientenabstieg genannt. Dabei wird der Gradient des Verlusts in Bezug auf die Gewichte berechnet. Der Gradient zeigt die Richtung des steilsten Anstiegs des Verlusts, und die Gewichte werden in die entgegengesetzte Richtung angepasst, um den Verlust zu verringern. Die Berechnung des Gradienten in neuronalen Netzen erfolgt durch Backpropagation, eine Anwendung der Kettenregel, die die Gradienten vom Ausgang zurück durch das Netzwerk propagiert. Die Lernrate (Learning Rate) bestimmt die Schrittgröße bei der Anpassung der Gewichte. Eine zu kleine Lernrate kann zu langsamer Konvergenz oder dem Steckenbleiben in lokalen Minima führen, während eine zu große Lernrate zu instabilem Verhalten führen kann. Adaptive Lernratenalgorithmen wie Adam passen die Lernrate während des Trainings dynamisch an.\n\n\nOptimierungstechniken\nDer stochastische Gradientenabstieg (SGD) berechnet den Gradienten nur für einen einzelnen zufällig ausgewählten Datenpunkt und ist daher schneller, aber verrauschter als der herkömmliche Gradientenabstieg, der den Gradienten über den gesamten Datensatz berechnet. Der Mini-Batch-Gradientenabstieg ist ein Kompromiss, bei dem der Gradient für eine kleine Menge (Batch) von Datenpunkten berechnet und gemittelt wird, was eine stabilere und effizientere Optimierung ermöglicht und die Parallelisierung auf GPUs erlaubt.\n\n\nOverfitting und Regularisierung\nOverfitting tritt auf, wenn ein Modell auf den Trainingsdaten sehr gut funktioniert, aber schlecht auf neuen, unbekannten Daten generalisiert. Dies geschieht, wenn das Modell die Trainingsdaten zu stark auswendig lernt. Regularisierungstechniken zielen darauf ab, Overfitting zu verhindern. Dropout ist eine Technik, bei der während des Trainings zufällig einige Aktivierungen von Neuronen auf Null gesetzt werden, um zu verhindern, dass sich das Netzwerk zu stark auf einzelne Neuronen verlässt. Early Stopping ist eine Methode, bei der der Trainingsprozess beendet wird, wenn die Leistung des Modells auf einem separaten Validierungsdatensatz (ein Proxy für Testdaten) zu sinken beginnt, obwohl der Verlust auf den Trainingsdaten weiterhin abnimmt.\n\n\nAusblick\nDie nächste Vorlesung wird sich mit Deep Sequence Modeling befassen, der Grundlage von Large Language Models."
  },
  {
    "objectID": "projects/mirth/mirth_step_by_step.html",
    "href": "projects/mirth/mirth_step_by_step.html",
    "title": "Install Mirth Connect (Step-by-step)",
    "section": "",
    "text": "In einem Projekt im 7. Semester arbeiten wir mit dem Kommunikationsserver Mirth Connect. Diese Anleitung zeigt einen möglichen Installationsprozess für Linux in einer VM."
  },
  {
    "objectID": "projects/mirth/mirth_step_by_step.html#voraussetzungen",
    "href": "projects/mirth/mirth_step_by_step.html#voraussetzungen",
    "title": "Install Mirth Connect (Step-by-step)",
    "section": "Voraussetzungen",
    "text": "Voraussetzungen\nDer Kommunikationsserver “Mirth® Connect by NextGen Healthcare” (kurz: Mirth) kann unter Linux, Windows oder iOS betrieben werden. Die offizielle Dokumentation von NextGen Healthcare zeigt den Installationsprozess mit Hilfe eines GUI oder CLI Installers.\nIn unserem späteren Projekt-Use-Case wird höchst wahrscheinlich Linux zum Einsatz kommen, sodass zunächst ein Linux System benötigt wird. Wir arbeiten hier mit einer virtuellen Maschine mit Debian. Für die virtuelle Maschine wird Virtual Box von Oracle verwendet. An dieser Stelle soll weder die Installation von Virtual Box noch von Linux in einer VM gezeigt werden. Dafür gibt es bereits diverse Ressourcen im Internet.\n\n\n\n\n\n\nWichtig: Netzwerkbrücke\n\n\n\nUm später auf Mirth und die VM zugreifen zu können, sollte bei (oder nach) dem Erstellen der VM die Netzwerkbrücke unter dem Netzwerkadaptern ausgewählt werden.\n\n\n\n\n\nNetzwerkbrücke\n\n\nOptional sollte das Linux System SSH unterstützen oder beim Installationsprozess sollte SSH ebenfalls installiert werden. Eine Anleitung für SSH findet man beispielweise hier. Mit SSH kann man von einem anderen Rechner direkt auf dem Linux System arbeiten (per Kommandozeile). Es kann dafür einfacher sein, am Rechner mit Internetbowser zu arbeiten und (z.B. für MS Windows) via PowerShell auf das Linux System zuzugreifen. Das spart durchaus Arbeit zum “Abtippen” von URLs oder Code, den man stattdessen einfach Copy&Paste vom Internet Brwoser ins Terminal kopieren kann.\n\n\n\nSSH installieren\n\n\nsudo apt install openssh-server\nAnstatt einer Linux VM könnten Windows-User alternativ WSL (Windows Subsystem for Linux) nutzen. Wenn Debian / Linux installiert wurde, können wir uns um die Installation von Mirth kümmern."
  },
  {
    "objectID": "projects/mirth/mirth_step_by_step.html#gleicht-geht-es-los",
    "href": "projects/mirth/mirth_step_by_step.html#gleicht-geht-es-los",
    "title": "Install Mirth Connect (Step-by-step)",
    "section": "Gleicht geht es los",
    "text": "Gleicht geht es los\nStartet das Linux System und öffnet das Terminal. Alternativ kann man sich via SSH in die Linux VM über den normalen Rechner einloggen. In meinem Beispiel habe ich mich mit MS Windows PowerShell auf Debian eingeloggt.\nssh mirth@debian\n\n\n\nMit Windows PowerShell und SSH auf Debian einloggen\n\n\nZunächst sollte das Linux System aktualisiert werden. Beim ersten Update muss man möglicherweise den eigenen User als Sudouser hinterlegn.\nsu\n[enter root password]\nsudo usermod -aG sudo [username]\nDie ausführliche Anleitung zeigt mehrere Varianten, um den User als Sudouser zu definieren.\nDanach wird die Systemaktualisierung durchgeführt.\nsudo apt update\nsudo apt upgrade\nDas Systemupdate findet man in vielen Anleitungen und scheint sinnvoll, aber nicht zwingend nötig zu sein."
  },
  {
    "objectID": "projects/mirth/mirth_step_by_step.html#mirth-herunterladen",
    "href": "projects/mirth/mirth_step_by_step.html#mirth-herunterladen",
    "title": "Install Mirth Connect (Step-by-step)",
    "section": "Mirth herunterladen",
    "text": "Mirth herunterladen\nDie Installation soll hier möglichst nur im Terminal erfolgen. Für den Download kann unter anderem wget genutzt werden. Das Programm muss möglicherweise zunächst noch intstalliert werden.\nsudo apt-get install wget\nDanach wird die neuste *tar.gz Datei heruntergeladen und als mirthconnect.tar.gz im eigenen Home-Ordner gespeichert. Man kann den Download natürlich auch in einen anderen Ordner herunterladen.\nwget -O mirthconnect.tar.gz https://s3.amazonaws.com/downloads.mirthcorp.com/connect/4.5.1.b332/mirthconnect-4.5.1.b332-unix.tar.gz\nDer Download startet, zeigt einen Fortschrittsbalken an und bestätigt den (hoffentlich) erfolgreichen Abschluss. Optional kann man manuell nachschauen, ob die Datei tatsächlich im Home-Verzeichnis angekommen ist.\nls\n\n\n\nDownload von Mirth und Kontrolle\n\n\nDamit ist der erste Schritt erledigt und die Installation von Mirth und Java kann folgen."
  },
  {
    "objectID": "projects/mirth/mirth_step_by_step.html#mirth-und-java-installieren",
    "href": "projects/mirth/mirth_step_by_step.html#mirth-und-java-installieren",
    "title": "Install Mirth Connect (Step-by-step)",
    "section": "Mirth und Java installieren",
    "text": "Mirth und Java installieren\nMirth benötigt Java, sodass dies zunächst installiert werden muss. Wie üblich, gibt es dafür mehrere Varianten, aber letztlich werden nur zwei Befehle benötigt.\nsudo apt install default-jre\nsudo apt install default-jdk\nDer Mirth-Download wird einfach entpackt, sodass ein weiterer Ordner Namens “Mirth Connect” erstellt wird. Dieser enthält bereit alle relevanten Dateien, die der Kommunikationsserver benötigt.\ntar xf mirthconnect.tar.gz\ncd Mirth\\ Connect/\nls\nMirth muss nur noch gestartet werden.\n./mcservice start\n\n\n\nStarten des Mirth Connect Service\n\n\nDer Status des Mirth Connect Service lässt sich einfach anzeigen.\n./mcservice status\n\n\n\nStatus des Mirth Connect Service\n\n\nInnerhalb des gleichen Netzwerkes kann nun mittels eines Webbrowsers auf Mirth zugegriffen werden. Eurer lokaler Rechner muss ebenfalls Java installiert haben. Die IP-Adresse des Linux Systems lässt sich leicht herausfinden.\nip a\n\n\n\nWo man die IP-Adresse findet\n\n\nDie IP-Adresse wird zusammen mit dem Port 8080 (mit Doppelpunkt voneinander getrennt), in die Browser-Adresszeile eingegeben. Der Mirth Startbildschirm wird gezeigt und man kann den Mirth Connect Administrator starten.\n\n\n\nBrowseransicht\n\n\n\n\n\n\n\n\nWirewall\n\n\n\nFalls noch eine Firewall (ufw: Ubuntu Firewall) aktiv ist, muss der entsprechende Port möglicherwiese geöffnet werden. Man kann beispielweise Port 8080 öffnen mit dem Befehl sudo ufw allow 8080.\n\n\nDer Weblaucher wird im Browser heruntergelden, installiert und gestartet. Gegebenfalls muss man die Installation auf dem lokalen Rechner bestätigen, wie man es von anderen Applikationen gewohnt ist. Je nach Netzwerk muss der Port 8843 noch freigegeben werden sudo ufw allow 8443.\nDas Login-Fenster öffnet sich mit der bereits hinterlegten Serverdaten. Mit admin als Username und admin als Passwort kann man sich initial einloggen in Mirth Connect Administrator.\n\nDie persönlichen Daten sowie ein neues Passwort wird im nächsten Schritt einmalig eingegeben. Danach ist Mirth Connect fertig installiert und bereit für mehr.\n\n\n\n\n\n\n\nPraktischer Hinweis\n\n\n\nEs könnte sinnvoll sein, den Mirth Connect Ordner umzubenennen, weil Leerzeichen evtl. gewisse spätere Aspekte verkomplizieren. Im Terminal mit dem Befehl mv Mirth\\ Connect/ mirthconnect ist die neue Ordnerbezeichnung leicht umgesetzt."
  },
  {
    "objectID": "projects/mirth/mirth_step_by_step.html#referenzen",
    "href": "projects/mirth/mirth_step_by_step.html#referenzen",
    "title": "Install Mirth Connect (Step-by-step)",
    "section": "Referenzen",
    "text": "Referenzen\nViele weitere Informationen über Mirth findet man in der offiziellen Dokumentation von NextGen Healthcare oder vielen weiteren Seiten / Foren sowie Video-Tutorials."
  },
  {
    "objectID": "projects/mirth/mirth_step_by_step.html#alternativen",
    "href": "projects/mirth/mirth_step_by_step.html#alternativen",
    "title": "Install Mirth Connect (Step-by-step)",
    "section": "Alternativen",
    "text": "Alternativen\nAlternativ zu einer Linux Installation kann man Mirth connect auch in einem Docker-Container betreiben. Wie das funktioniert, wird im Mirth Project Forum gezeigt. Zudem lässt sich auf PostgreSQL und PG-Admin in einem Container betreiben. Wenn Ihr noch keine Erfahrung mit Docker habt, so gibt es viele Tutorials zu diesem Thema."
  },
  {
    "objectID": "projects/mathe2/Mathe2Notebook.html",
    "href": "projects/mathe2/Mathe2Notebook.html",
    "title": "Jupiter Notebooks und Quarto",
    "section": "",
    "text": "Eigentlich wollte ich Python besser zu lernen und Jupyter Notebook dafür nutzen. Die vorbereitete .ipynb Datei wird automatisch in die Quarto-Website eingebunden. Nice!"
  },
  {
    "objectID": "projects/mathe2/Mathe2Notebook.html#random-code",
    "href": "projects/mathe2/Mathe2Notebook.html#random-code",
    "title": "Jupiter Notebooks und Quarto",
    "section": "Random Code",
    "text": "Random Code\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n# Generate 100 random data points along 3 dimensions\nx, y, scale = np.random.randn(3, 100)\nfig, ax = plt.subplots()\n# Map each onto a scatterplot we'll create with Matplotlib\nax.scatter(x=x, y=y, c=scale, s=np.abs(scale)*500)\nax.set(title=\"Some random data, created with JupyterLab!\")\nplt.show()\n\n\n\n\n\n\n\n\nIch werde aber noch Python und Quarto in R Studio ausprobieren und danach entscheiden, welche Variante für mich besser geeignet ist. On va voir…"
  },
  {
    "objectID": "projects/es kann losgehen/es kann losgehen.html",
    "href": "projects/es kann losgehen/es kann losgehen.html",
    "title": "Es kann losgehen",
    "section": "",
    "text": "Am 07.02.2024 habe ich mich erstmals mit Quarto beschäftigt. Ursprünglich wollte ich es im sechsten Semester nur “nebenbei” ausprobieren und ein paar Erfahrungen sammeln. Aus dieser Idee ist ein kleines Projekt entstanden. So romantisch wie im KI generierten Foto von Adobe Firefly waren meine ersten Quarto-Abende zwar nicht. Aber ein schöner Gedanke ist es …\n\n\n\nAdobe Firefly: An einem See sitzend arbeite ich am Laptop und im Hintergrund geht die Sonne unter\n\n\nWeitere Infos findet Ihr auf der offiziellen Quarto-Seite"
  },
  {
    "objectID": "projects/atc/parseATC.html",
    "href": "projects/atc/parseATC.html",
    "title": "Webscraping mit Python",
    "section": "",
    "text": "Der Index der anatomischen, therapeutischen und chemischen Strukturen (ATC) wird von der WHO resp. dem norwegischen Institut für Public Health verwaltet und veröffentlicht. Die jährlich aktualisierten Daten sind im öffentlich verfügbaren ATC-Index auf der Website einsehbar. Der manuelle Websiteaufruf ist für gewisse Usecases nicht sinnvoll. Hier wird eine Möglichkeit gezeigt, wie das Webscraping resp. Parsen der ATC-Codes und deren Beschreibung mit Python umgesetzt werden kann. Der Output ist eine csv-Datei mit den drei pipegetrennten Spalten atc_name, atc_code, level.\nZunächst werden die relevanten Packages geladen. Mit BeautifulSoup kann das retournierte HTML-Skript bearbeitet werden.\n\nimport numpy as np\nimport pandas as pd\nimport datetime\nfrom bs4 import BeautifulSoup\nimport urllib.request\nimport re\n\nWenn man den ATC-Index-Wesite betrachtet und die Seite analysiert, erkennt man schnell die URL-Struktur. Dem Endpunkt wird im ersten Parameter code der gesuchte ATC-Code übergeben. Mit einem zweiten Parameter kann man die ausführliche Beschreibung anzeigen lassen, die hier nicht benötigt wird und auf no gesetzt ist.\n\nendpoint = \"https://atcddd.fhi.no/atc_ddd_index\"\npara1 = \"/?code=\"\npara2 = \"&showdescription=no\"\n\nDie Hilfsfunktion extract_value dient später dazu, die ATC-Codes und deren Schreibung aus eine Liste (vom DOM-Elementen) zu extrahieren.\n\ndef extract_value(s):\n    return s.split('=')[1].split('&')[0]\n\nDer Hauptfunktion parseATC wird ein ATC-Code-Parameter übergeben Diese Funktion wird auf ein iterierbares Dataframe angewandt und somit später der komplette Index durchforstet. Im ersten Teil dieser Funktion wird die Request-URL zusammengesetzt, abgefragt und die “html-Suppe” gekocht. Der DOM des ATC-Index ist so aufgebaut, dass die ATC-Codes und die Beschreibung aus A-Elementen herausgelesen werden können, in denen der Begriff code vorkommt. Die extrahierten Inhalte werden in einem lokalen Dataframe zwischengespeichert und dort das Level (String-Länge) des ATC-Codes ermittelt.\n\ndef parseATC(atc_as_para):\n  # Parse HTML content\n  url = endpoint + para1 + atc_as_para + para2\n  html_content = urllib.request.urlopen(url).read()\n  soup = BeautifulSoup(html_content, \"html.parser\")\n  \n  # Find all \"a\" elements in DOM with \"code\" inside\n  codes = soup.find_all(href=re.compile(\"code\"))\n  \n  # Extract the data\n  atc_names = [a.text for a in codes]\n  href_string = [str(a.attrs) for a in codes]\n  \n  # Create local data frame\n  data = {\"atc_name\": atc_names, \"href\": href_string}\n  df_data = pd.DataFrame(data)\n  \n  # Extract atc code from href and delete href column\n  df_data['atc_code'] = df_data['href'].apply(lambda x: extract_value(x))\n  del df_data['href']\n  df_data = df_data[df_data['atc_name']!='Show text from Guidelines']\n  \n  # calculate atc-level (length of atc code)\n  df_data['level'] = df_data['atc_code'].apply(lambda x: len(x))\n  print(datetime.datetime.now()) # just showing script is running\n  return df_data\n\nZwei initiale Dataframes werden erstellt. Über df wird später iteriert und in df_combined werden alle Ergebnisse gespeichert.\n\ndf_combined = df = pd.DataFrame()\ndf_combined = df = parseATC(\"\")\n\nNun folgt die zentrale Abfrage aller Daten. Die Levels 1, 3, 4 und 5 werden itrerativ abgefragt und die parseATC-Funktion nach und nach auf alle Codes und Subcodes angewendet. Diese doppelte Loop dauert rund 5 Minuten, wobei der innere Loop hier am wichtigsten ist.\n\nlevel_iterator = np.array([1,3,4,5])\n\nfor li in level_iterator:\n  print(\"level: \"+str(li))\n  df = df_combined[df_combined.level==li]\n  \n  for row in df.itertuples():\n    df_data = parseATC(row.atc_code) # global data frame\n    df_combined = pd.concat([df_combined, df_data], ignore_index=True)\n  \n  # drop duplicates \n  df_combined = df_combined.drop_duplicates()\n\nZu Guter letzt werden die Daten sortiert nach dem ATC-Code und die finale Liste als atc_list.csv exporiert.\n\ndf_combined = df_combined.sort_values('atc_code')\ndf_combined.to_csv('atc_list.csv', index=False, sep='|')\n\nBei diesem Thema sollte man jedoch die Lizenzbestimmungen beachten. Der rechtliche Aspekt wurde hier nicht geklärt, sondern nur die technsiche Machbarkeit dargestellt.\nHier findet man weitere Infos zu BeautifulSoup."
  },
  {
    "objectID": "old.html",
    "href": "old.html",
    "title": "Frühere Projekte",
    "section": "",
    "text": "Bevor ich mit Quarto begann, hatte ich andere kleine Projekte ausprobiert."
  },
  {
    "objectID": "old.html#living-case-eyewin-interfaces",
    "href": "old.html#living-case-eyewin-interfaces",
    "title": "Frühere Projekte",
    "section": "Living Case: EyeWin Interfaces",
    "text": "Living Case: EyeWin Interfaces\n\n\nSchnittstelle von der Patientenverwaltung Eyewin zu drei Diagnostikgeräten: diese Arbeit wurde im Studienmodul «Living Case 1» während des Herbstsemesters 2023/24 erstellt. Projektpartner und Auftraggeberin war die Firma Eschmann Contactlinsen AG in Bern, die optometrische Dienstleistungen anbietet.\nDetails"
  },
  {
    "objectID": "old.html#analyse-von-sensordaten-über-stürze",
    "href": "old.html#analyse-von-sensordaten-über-stürze",
    "title": "Frühere Projekte",
    "section": "Analyse von Sensordaten über Stürze",
    "text": "Analyse von Sensordaten über Stürze\n\n\n\n\n\n\nIm vierten Semester besuchten wir das Modul “R”. Dort durften wir ein R Markdown Dokument zum Thema “Simulated Falls and Daily Living Activities” erstellen. Der Datensatz wurde von A.T. Özdemir und B. Barshan erstellt.\nDetails"
  },
  {
    "objectID": "old.html#bi-in-healthcare",
    "href": "old.html#bi-in-healthcare",
    "title": "Frühere Projekte",
    "section": "BI in Healthcare",
    "text": "BI in Healthcare\n\n\nIm fünften Semester lernen wir weiteres über den Einsatz von R im Bereich Business Intelligence. Der Dozent führte den Unterricht als Flipped Classroom durch, sodass wir viele Haus- und Übungsaufgaben sowie Selbststudium absolvierten. Die erarbeiteten Ergebnisse habe ich als R Markdown zusammengefasst.\nDetails"
  },
  {
    "objectID": "old.html#swissdrg-wordcloud",
    "href": "old.html#swissdrg-wordcloud",
    "title": "Frühere Projekte",
    "section": "SwissDRG Wordcloud",
    "text": "SwissDRG Wordcloud\n\n\n\n\n\n\nAls Teil des Moduls BI In Healthcare hatten wir uns mit Word-Clouds beschäftigt. Für meine Arbeitskollegen habe ich nun eine Wortcloud mit den Bewgriffen des Definitionshandbuch der SwissDRG Version 13.0 erstellt.\nDetails"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Christian Franke",
    "section": "",
    "text": "Als Volkswirt habe ich zwölf Jahre für Unfall- und Krankenversicherungen im Bereich der Datenanalyse und Controlling gearbeitet. Das war sehr lehrreich und interessant, weil ich an den Schnittstellen zwischen IT und Fachabteilungen tätig war. Dennoch wollte ich mich nochmals weiterentwickeln. Und so kam es, dass ich ein Studium für Medizin-Informatik begann.\n\nAus- und Weiterbildungen\n2021-2025: Bachelor-Studium für Medizin-Informatik an der Berner Fachhochschule (in Teilzeit)\n2018: CAS in Gesundheitsökonomie und gesundheitsökonomische Evaluation (Public Health Weiterbildung der Universitäten Basel, Bern und Zürich)\n2004-2009: Diplom-Studium für Volkswirtschafslehre an der Julius-Maximilians-Universität Würzburg\n\n\nBerufliche Tätigkeiten\nSeit 2021: SwissDRG AG, Bern\n2020/2021: Valiant Bank AG, Bern\n2016-2019: Sanitas Krankenversicherung AG, Zürich\n2013-2015: Basler Versicherungen AG, Basel\n2009-2015: Berufsgenossenschaft Holz und Metall, Mainz\n\n\nTechnische Skills\n\nNeo4j (Neo4j GraphAcademy Zertifikate)\nMS Excel (Experte)\nSQL (Experte)\nBI Tools (umfangreich)\n\nMicroStrategy\nPower BI\n\nR, Quarto, Shiny (regelmässiger Einsatz)\nJava, Python (praktische Erfahrung)\nausserdem\n\nDatenvisualisierung\nProjektmanagement"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Grüessech, hallo, salut & hello",
    "section": "",
    "text": "Für die meisten Menschen ist Programmieren kein “Hobby”. Und auch ich verbringe meine Zeit lieber Outdoor und sause mit dem Kite und Foil über das Wasser. Trotzdem muss ich gestehen: ein wenig Spass macht IT schon und ausserdem sind die Berufsaussichten rosig. Oder wie ich gern sage: das Investment im mein eigenes Humankapital hat mir bisher am meisten Rendite eingebracht.\nDeshalb möchte ich Quarto besser kennenlernen und etwas Praktisches machen: eine kleine Website auf der ich einige meiner Projekte präsentieren kann.\nViel Spass beim Stöbern."
  },
  {
    "objectID": "poster.html",
    "href": "poster.html",
    "title": "Poster",
    "section": "",
    "text": "Zugegeben, diese Poster wurden bei keiner einzigen Konferenz präsentiert. Wir dürften sie “nur” unseren Dozenten und Kommilitonen erläutern. Aber was nicht ist, kann ja noch werden…\n\n\n\n\nMedikamenten-Graph-Datenbank\n\n\n\n\n\n\n\n\nSchnittstellen\n\n\n\n\n\n\n\n\n\n\nTelemedizin\n\n\n\n\n\n\n\n\nVirtual Reality"
  },
  {
    "objectID": "projects/book/mal ein buch schreiben.html",
    "href": "projects/book/mal ein buch schreiben.html",
    "title": "Einfach Schriftsteller werden",
    "section": "",
    "text": "Nach dem Motto “Ein Blatt, richtig beschriftet, macht aus Pennern Millionäre” von Blumentopf schreibe ich nun ein Buch, ein Quarto-Buch.\nZugegeben, ein grosser Schriftsteller werde ich wohl nicht. Ich möchte nur die Unterlage des Moduls “Mathematics for data analysis” zusammenfassen und gleichzeitig Python üben. Der Vorteil eines Buches ist die Unterteilung in Kapitel und ausserdem könnte ich R nutzen, falls ich mit Python nicht weiterkomme. Let’s go!\n\n\n\nCover: Blumentopf - Grosses Kino\n\n\nWeitere Infos findet Ihr auf der offiziellen Quarto-Seite"
  },
  {
    "objectID": "projects/interops/interops.html",
    "href": "projects/interops/interops.html",
    "title": "Interoperabilität",
    "section": "",
    "text": "..DER Podcast rund um Gesundheits- und Medizininformatik hat einen Überblick über Interoperabilität gegeben. Die wichtigste Begriffe werden hier zusammengestellt.\n\n\n\n\n\nmindmap\n  root((Interops))\n    Systeme\n      Terminologieserver\n      Kommunikationsserver\n      KIS\n      RIS\n      LIS\n      PACS\n      CDR\n      PIS\n    Standards\n      openEHR\n        Maximal ausdefiniert\n        Archetyp\n      HL7 V2\n        Nachrichtentypen\n      FHIR\n        Ressource\n        80/20\n      DICOM\n        Worklist\n      IHE\n        XDS\n      CDA\n      EPD\n    Arten\n      Semantisch\n        Annotation\n        Ordnungssyteme\n        Klassifikation\n        Wichtig\n          ICD\n          Snomed CT\n          NANDA\n          NIC\n          NOC\n          LOINC\n          UCUM\n          CHOP\n          ATC\n      Syntaktisch\n        xDT\n\n\n\n\n\n\nDabei handelt es sich um ein Mermaid-Diagramm (Mindmap). Weitere Infos zu Mermaid findet Ihr unter https://mermaid.js.org."
  },
  {
    "objectID": "projects/medi-graph/medi-graph.html",
    "href": "projects/medi-graph/medi-graph.html",
    "title": "Medikamenten-Graph",
    "section": "",
    "text": "Ein Graph-Modell mit Medikamenten-Daten in der Graph-Datenbank Neo4j war das Ergebnis meiner Bachelor-Thesis."
  },
  {
    "objectID": "projects/medi-graph/medi-graph.html#summary",
    "href": "projects/medi-graph/medi-graph.html#summary",
    "title": "Medikamenten-Graph",
    "section": "Summary",
    "text": "Summary\nDie Graph-Datenbank mit öffentlich verfügbaren Informationen über hochteure Medikamente soll der SwissDRG AG helfen, Zusammenhänge zwischen Medikamenten und deren Indikationen einfacher zu erkennen. Die Daten wurden in Neo4j integriert und sollen bei der Weiterentwicklung der stationären Tarifsysteme im Schweizer Gesundheitswesen unterstützen. Ein kurzes Video gibt einen kompakten Einblick in das Projekt."
  },
  {
    "objectID": "projects/medi-graph/medi-graph.html#problemstellung",
    "href": "projects/medi-graph/medi-graph.html#problemstellung",
    "title": "Medikamenten-Graph",
    "section": "Problemstellung",
    "text": "Problemstellung\nDie Anzahl hochteurer Medikamente ist in den vergangenen Jahren deutlich gestiegen. Dabei ist es herausfordernd, ähnliche Indikationen für unterschiedliche Substanzen und die Verbindungen zwischen den Medikamenten im Überblick zu behalten. Besonders Graph-Datenbanken sind geeignet, um genau solche Relationen abzubilden. Mit Hilfe von Neo4j sollte ein Graph-Datenmodell erstellt werden. Die Daten stammen von der SwissDRG AG, der Spezialitätenliste (SL) des BAG, dem ATC-Index der WHO, der Europäischen Arzneimittelagentur sowie den strukturierten Arzneimittelinformationen der Stiftung Refdata. Dabei sollte es möglich sein, die öffentlichen Daten monatlich zu aktualisieren und bei Bedarf weitere Informationsquellen zu integrieren."
  },
  {
    "objectID": "projects/medi-graph/medi-graph.html#ergebnis",
    "href": "projects/medi-graph/medi-graph.html#ergebnis",
    "title": "Medikamenten-Graph",
    "section": "Ergebnis",
    "text": "Ergebnis\nDie Medikamentendaten wurden in 13 Labels gespeichert, die mit 14 unterschiedlichen Relationen-Typen miteinander verbunden sind. Es wurden über 27’000 Knoten und über 41’000 Relationen kreiert. Die Schemalosigkeit und Flexibilität von Graph-Datenbanken war speziell für die Knoten Substanz und Produkt wichtig. Zwischen diesen Knoten wurde die Relation HAT_ATC auch dann erstellt, wenn sich deren direkte ATC-Codes zwar unterschieden, aber gleichzeitig der indirekte ATC-Code der SL übereinstimmte. Defizitäre Datenqualität kann damit teilweise ausgeglichen werden. Die Datenbank wird in der SwissDRG AG zukünftig in einer Cloud-Umgebung eingesetzt, um beispielsweise Substanzen mit «gleicher» Indikation einfacher zu identifizieren und das Tarifsystem gezielt weiterzuentwickeln.\n\n\n\nMedikamenten Datenmodell"
  },
  {
    "objectID": "projects/medi-graph/medi-graph.html#details",
    "href": "projects/medi-graph/medi-graph.html#details",
    "title": "Medikamenten-Graph",
    "section": "Details",
    "text": "Details\nDen Source-Code findet Ihr in meinem GitHub-Repo. Wenn Ihr Details wissen möchtet, könnt ihr diese im Abschlussbericht in Ruhe nachlesen."
  },
  {
    "objectID": "projects/mirth autostart/mirth autostart.html",
    "href": "projects/mirth autostart/mirth autostart.html",
    "title": "Mirth starten nach dem Booten",
    "section": "",
    "text": "Nachdem Mirth Connect grundsätzlich in Debian installiert ist, soll es automatisch starten nach dem Booten. Dafür kann eine Service-Datei erstellt werden. Falls Euch das Thema sehr interessiert, könnt in z.B. diese ausführliche Einfühung lesen."
  },
  {
    "objectID": "projects/mirth autostart/mirth autostart.html#die-service-datei",
    "href": "projects/mirth autostart/mirth autostart.html#die-service-datei",
    "title": "Mirth starten nach dem Booten",
    "section": "Die Service-Datei",
    "text": "Die Service-Datei\nZunächst muss eine .service Datei erstellt werden. Diese kann man zunächst im Home-Verzeichnis erstellen (mit touch). Später wird die Datei in ein System-Verzeichnis verschoben. Die .service Datei kann mit dem Terminal-Befehl nano bearbeitet werden.\ntouch mirthconnect.service\nnano mirthconnect.service\nIm Bearbeitungsmodus kann folgendes Skript hineinkopiert werden (Quelle und weitere Details).\n[Unit]\nDescription=MirthConnect\nAfter=network.target\n\n[Service]\nType=forking\n\nUser=mirth\nGroup=mirth\nExecStart=/home/mirth/mirthconnect/mcservice start\nExecStop=/home/mirth/mirthconnect/mcservice stop\nExecRestart=/home/mirth/mirthconnect/mcservice restart\n\nTimeoutSec=60\n\n[Install]\nWantedBy=multi-user.target\nMit Ctrl + x verlässt man den Bearbeitungmodus und speichert die Änderung mit y ab. Die erstellte Datei mirthconnect.service muss nun mit Admin-Privilegien in einen Systemordner verschoben werden. Wir wechseln zunächst zum su User.\nsu\n[Password:]\nDanach wählen wir das Zielverzeichnis aus.\ncd /usr/lib/systemd/system\nDie mirthconnect.service Datei wird mit cp in das Zielverzeichnis verschoben. Der Punkt am Ende der Zeile unten gehört noch zum Befehl dazu und sollte nicht vergessen werden.\ncp /home/mirth/mirthconnect.service .\nDer Autostart kann aktiviert werden.\nsystemctl enable mirthconnect.service\nNach einem Reboot sollte Mirth Connect regulär laufen, sodass man sich via Webbrowser oder Mirth Connect Administrator in den Kommunikationsserver einloggen kann.\n\n\n\n\n\n\nsu Password vergessen?\n\n\n\nFalls das su Passwort vergessen ging, diese Anleitung hilft beim Zurücksetzen."
  },
  {
    "objectID": "projects/node25.html",
    "href": "projects/node25.html",
    "title": "Nodes 2025",
    "section": "",
    "text": "Yeah, ich darf meine wichtigsten Erkenntnisse der Bachelorthesis bei der Nodes 2025 online Konferenz präsentieren."
  },
  {
    "objectID": "projects/R API und FHIR/r api fhir.html",
    "href": "projects/R API und FHIR/r api fhir.html",
    "title": "RestAPI mit R",
    "section": "",
    "text": "Als vorerst letztes Mini-Projekt frage ich mit R die Daten eines FHIR-Test-Servers ab und erstelle dort einen Patienten. In nächster Zeit werde ich mich wieder auf das Frühlingssemester konzentieren und habe vermutlich keine Zeit mehr für diese kleine Quarto-Website. Es hat aber Spass gemacht."
  },
  {
    "objectID": "projects/R API und FHIR/r api fhir.html#hintergrund",
    "href": "projects/R API und FHIR/r api fhir.html#hintergrund",
    "title": "RestAPI mit R",
    "section": "Hintergrund",
    "text": "Hintergrund\nIm Rahmen einer Seminararbeit zum Thema Schnittstellen habe ich unter anderem eine FHIR-Patienten-Ressource erstellt und diese zu Lern- und Entwicklungszwecken an einen FHIR-Test-Server geschickt. Damals verwendete ich Postman und Java."
  },
  {
    "objectID": "projects/R API und FHIR/r api fhir.html#vorbereitung",
    "href": "projects/R API und FHIR/r api fhir.html#vorbereitung",
    "title": "RestAPI mit R",
    "section": "Vorbereitung",
    "text": "Vorbereitung\nIn R kann man mit dem Package “httr2” auf RestAPI zugreifen. Das Paket httr2 wurden von Hadley Wickham entwickelt und ersetzt httr.\n\nlibrary(httr2)\n\nZunächst definieren wir den Server sowie den Endpunkt (hier “Patient”).\n\nurl &lt;- \"https://hapi.fhir.org/baseR5/\"\nreq &lt;- request(url) %&gt;% \n  req_url_path_append(\"Patient\")"
  },
  {
    "objectID": "projects/R API und FHIR/r api fhir.html#get",
    "href": "projects/R API und FHIR/r api fhir.html#get",
    "title": "RestAPI mit R",
    "section": "Get",
    "text": "Get\nDa wir nur einen Patienten nach Namen (hier: “Franke”) suchen möchten, fügen wir den Namen als Variable “name_str” dem Request hinzu. Mit req_perform() führen wir die Abfrage aus.\n\nname_str &lt;- \"Franke\"\nget_request &lt;- req %&gt;%\n  req_url_query(name=name_str) %&gt;% \n  req_perform()\n\nDen Status können wir uns anzeigen lassen und hoffen, dass er 200 ist.\n\nget_request %&gt;% resp_status()\n\nDie Antwort des Resquest erhalten wir in JSON Format und speichern diese als get_response ab. Auf die einzelnen Elemente (z.B. den Vornamen “given”) können wir mit den üblichen Listen-Operationen zugreifen.\n\nget_response &lt;- resp_body_json(get_request)\nget_response$entry[[1]]$resource$name[[1]]$given\n\nFalls wir nicht einen, sondern alle Patienten suchen, können wir uns natürlich auch auf alle Patientendaten zugreifen. Dazu lassen wir einfach den req_url_query() Befehl weg.\n\nget_all_patients &lt;- req %&gt;% \n  req_perform() %&gt;% \n  resp_body_json()"
  },
  {
    "objectID": "projects/R API und FHIR/r api fhir.html#delete",
    "href": "projects/R API und FHIR/r api fhir.html#delete",
    "title": "RestAPI mit R",
    "section": "Delete",
    "text": "Delete\nDer Aufbau der httr2-Anfragen ist eigentlich immer das “Gleiche”. Wir nehmen den “Basis-Request” (hier: req), fügen Parameter ein (hier: req_url_query), wählen eine Resquest-Methode (hier: req_method) und lassen die Abfrage laufen (hier: req_perform).\n\nreq %&gt;%\n  req_url_query(name=name_str) %&gt;%\n  req_method(\"DELETE\") %&gt;% \n  req_perform()\n\nNun wurde der Patient “Franke” gelöscht. Dies können wir kontrollieren, in dem wir den oben durchgeführten GET-Request nochmals ausführen und keinen entsprechenden Patienten mehr angezeigt erhalten.\n\nIn httr2 ist standardmässig die GET Methode voreingestellt. Mit req_method() können wir die Methode ändern, wie hier zu “DELETE”. Falls der Request nicht direkt ausgeführt, sondern nur “simuliert” werden soll, kann man statt req_perform() die Methode req_dry_run() nutzen."
  },
  {
    "objectID": "projects/R API und FHIR/r api fhir.html#post",
    "href": "projects/R API und FHIR/r api fhir.html#post",
    "title": "RestAPI mit R",
    "section": "Post",
    "text": "Post\nZu guter Letzt erstellen wir eine Patienten. Die FHIR-Patienten-Ressource wird zunächst (als Liste) aufgebaut.\n\npost_request_body &lt;- list(\n  resourceType= \"Patient\",\n  identifier= list(\n    use= \"official\",\n    value= \"BE1603301284\"\n  ),\n  name= list(\n    use= \"official\",\n    family= \"Franke\",\n    given= list(\"Christian\")\n  ),\n  gender= \"male\",\n  birthDate= \"1984-01-01\"\n)\n\nAlternativ können wir die Ressource aus einer Datei laden.\n\nlibrary(jsonlite)\npost_request_body &lt;- fromJSON(\"patient.fhir.json\")\n\nDer Request-Body wird nun einfach als JSON dem Reqeust übergeben (req_body_json(post_request_body)) und die Abfrage ausgeführt.\n\npost_request &lt;- req %&gt;%\n  req_body_json(post_request_body) %&gt;% \n  req_perform()\n\nFalls der Status 201 ist, dürfen wir uns freuen, denn 201 bedeutet “created”.\n\npost_request %&gt;% resp_status()"
  },
  {
    "objectID": "projects/R API und FHIR/r api fhir.html#fazit",
    "href": "projects/R API und FHIR/r api fhir.html#fazit",
    "title": "RestAPI mit R",
    "section": "Fazit",
    "text": "Fazit\nMit R und dem httr2 Package lassen sich RestAPI Request einfach durchführen. So kann man Patienten auf einem FHIR-Server abfragen, erstellen, löschen etc. Das Wichtige ist dabei lediglich, dass man sich an das FHIR-Format hält.\nDas soll nun der vorerst letzte Quarto-Beitrag gewesen sein. Ich werden mich in den kommenden Wochen auf meine Semester-Vorlesungen konzentrieren und freue mich auf Web & Netzwerke sowie Interoperabilität. Vielleicht bietet sich die Gruppenarbeit zu “SMART on FHIR” in Interops an, “R on FHIR” aufzugreifen, denn dafür schein es ein entsprechendes Paket zu geben.\nA bien-tôt…"
  },
  {
    "objectID": "projects/rsa/RSA mit modpower.html",
    "href": "projects/rsa/RSA mit modpower.html",
    "title": "RSA und Modulo-exponentiation",
    "section": "",
    "text": "Im Modul Informationssicherheit lernten wir unter anderem den RSA Schlüsselaustausch kennen. Dieses asymetrische kryptographische Verfahren ist eine wichtige Basis für den verschlüsselten Datenaustausch in der vernetzen Welt. Für effiziente Berechnungen der Schlüssel wird die Moduloexponentiation genutzt. Beides wird hier kurz erläutert und der R-Code präsentiert für ein einfaches Beispiel."
  },
  {
    "objectID": "projects/rsa/RSA mit modpower.html#hintergründe",
    "href": "projects/rsa/RSA mit modpower.html#hintergründe",
    "title": "RSA und Modulo-exponentiation",
    "section": "Hintergründe",
    "text": "Hintergründe\n\nRSA: Rivest, Sharmi, Adleman\n1977 publizierte\nasymetrisches kryptographisches Verfahren\nPrivater Schlüssel zum Entschlüsseln und Signieren (bleibt geheim)\nÖffentlicher Schüssel zum Verschlüsseln und Verifizieren"
  },
  {
    "objectID": "projects/rsa/RSA mit modpower.html#grundidee",
    "href": "projects/rsa/RSA mit modpower.html#grundidee",
    "title": "RSA und Modulo-exponentiation",
    "section": "Grundidee",
    "text": "Grundidee\n\nAlice generiert ein Schlüsselpaar (privat/public)\nprivat bleibt immer lokal bei Alice\npublic kann an Bob (oder die ganze Welt) gesendet werden\nBob nutzt den öffentlichen Schlüssel von Alice, verschlüsselt ein Dokument und sendet es an Alice\nAlice kann das Dokument mit dem privaten Schlüssel entschlüsseln"
  },
  {
    "objectID": "projects/rsa/RSA mit modpower.html#mathematisch",
    "href": "projects/rsa/RSA mit modpower.html#mathematisch",
    "title": "RSA und Modulo-exponentiation",
    "section": "Mathematisch",
    "text": "Mathematisch\nEs gilt\n\\[(m^e)^d \\bmod n = m\\]\nwobei der öffentliche Schlüssel \\((e,n)\\) enthält und der private Schlüssel \\((d,n)\\). D.h. die Exponenten sind Teile des privaten oder öffentlichen Schlüssls. \\(n\\) ist in beiden Schlüsseln enthalten und \\(m\\) ist die Nachricht.\nAlice erzeugt nun das Schlüsselpaar. Für den Modulo nutzt sie das Produkt aus zwei zufällig ausgewählte grosse Primazahlen \\(p\\) und \\(q\\).\n\\[n = p*q\\]\n\np = 11\nq = 13\nn = p*q\nn\n\n[1] 143\n\n\nDanach wird die Eulersche Phi-Funktion \\(\\varphi\\) ermittelt, die zur Ermittlung des privaten Schüssels benötigt wird. Dies ist somit ein wichtiger Zwischenschritt, aber Phi wird nicht weiter verwendet.\n\\[\\varphi = (p-1)*(q-1)\\]\n\nphi = (p-1)*(q-1)\nphi\n\n[1] 120\n\n\nWeiter wird eine teilerfremde Zahl \\(e\\) definiert (Exponent im öffentlichen Schlüssel), die zwischen 1 und \\(\\varphi\\) liegt. Wie \\(e\\) exakt festgelegt wird, habe ich in R jedoch nicht weiter berechnet resp. verifiziert.\n\ne = 23\n\nAls letztes muss noch die Zahl \\(d\\) (Exponent im privaten Schlüssel) ermittelt werden. Für \\(d\\) gilt\n\\[e*d \\bmod \\varphi = 1\\]\n\nfun_d &lt;- function(x, y) {\n  d &lt;- 1\n  while ((x*d) %% y != 1) {\n    d &lt;- d + 1\n  }\n  return(d)\n  }\nd &lt;- fun_d(e, phi)\nd\n\n[1] 47\n\n\nSomit haben wir die drei komponenten des öffentlichen und privaten Schlüssels festgelegt.\n\n\n[1] \"public key = (23,143)\"\n\n\n[1] \"private key = (47,143)\"\n\n\nEs gibt nun zwei Use-Cases, um die Schlüssel zu nutzen. Für beide Use-Cases wird die Moduloexponentiation verwendet, d.h. mit grossen Zahlen gerechnet. Hierfür benötigen wir in R die Library “numbers”.\n\nVerschlüsseln und Entschlüsseln von Nachrichten\nDie Nachricht \\(m\\) wird zunächst ASCII Codiert, d.h. es sind nur noch Zahlen vorhanden, mit denen wir die mathematischen Operationen durchführen können. Wir möchten beispielsweise den Buchstaben “S” verschlüsseln, was in ASCII der Zahl 83 entspricht.\n\nlibrary(numbers)\nm &lt;- 83\n\nUm die Nachricht zu verschlüsseln, nutzt Bob nun den öffentlichen Schlüssel.\n\\[c = m^e \\bmod n\\]\n\nc &lt;- modpower(m, e, n)\nc\n\n[1] 73\n\n\nAlice verwendet nun den privaten Schlüssel, um die Nachricht wieder lesbar zu machen.\n\\[m' = c^d \\bmod n \\]\n\nm_ &lt;- modpower(c, d, n)\nm_\n\n[1] 83\n\n\n\n\nSignieren und Verifizieren von Nachrichten\nEs ist wichtig, dass Nachrichten auf dem Weg zum Kommunikationspartner nicht verändert werde. Beispielweise könnte Alice die Nachricht (z.B. Überweisung) an Bob (z.B. Bank) signieren und Bob kann die Echtheit der Nachticht verifizieren.\nAlice signiert die Nachticht “42” mit Ihrem privaten Schlüssel.\n\\[s=m^d \\bmod n\\]\n\ns &lt;- 42\ns &lt;- modpower(m, d, n)\n\nBob nutzt den öffentlichen Schlüssel von Alice und wendet den öffentlichen Schlüssel auf die Signatur an. Wenn die Signatur und die Verifizierung identisch sind, ist die Nachticht unverändert.\n\\[v=s^e \\bmod n \\equiv m\\]\n\nv &lt;- modpower(s, e, n)\nprint(paste0(\"m is \",ifelse(v!=m, \"not valid\", \"valid\")))\n\n[1] \"m is valid\"\n\n\nDas ganze funktioniert in der Realität mit sehr grosen Zahlen. Die Moduloexponentiation kann beispielsweise mit dem “Square-and-multiply” Algorithmus schnell und einfach ausgeführt werden."
  },
  {
    "objectID": "projects/rsa/RSA mit modpower.html#square-and-multiply",
    "href": "projects/rsa/RSA mit modpower.html#square-and-multiply",
    "title": "RSA und Modulo-exponentiation",
    "section": "Square-and-multiply",
    "text": "Square-and-multiply\nDer Square-and-multiply Algorithmus und verwandte Methoden (z.B. Sliding Window) ermöglichen eine schnelle und einfache Berechnung der Modulo einer Exponentiation. Dabei gilt\n\\[m^e \\bmod n = (m \\bmod n)^e \\bmod n\\]\nDies hat praktische Auswirkung, weil wir mit “kleinen” Zahlen rechnen können. Zudem wird der Exponent in Binärzahlen umcodiert und bei jeden Zwischenschritt quadiert und, falls der bit eine 1 ist, auch noch mit der Basis multipliziert. Somit reduziert sich die Laufzeit des Algortihmus.\n\nHier noch die Prüfung in R.\n\nmodpower(3, 1955, 51)\n\n[1] 27"
  },
  {
    "objectID": "projects/zeppelin/zeppelin.html",
    "href": "projects/zeppelin/zeppelin.html",
    "title": "Quarto Präsentationen",
    "section": "",
    "text": "Im Modul Advanced Data Management durfte ich einen Vortrag über Apache Zeppelin halten. Die Präsentation habe ich zur Abwechselung mit Quarto/RStudio als reveal.js Dokument erstellt. Hier fasse ich die praktischen Erkenntnisse zusammen.\n\nMit Quarto lassen sich moderne Präsentationen erstellen, die responsiv sind und auf jedem Browser anschaut werden können.\nEs gibt eine umfangreiche Dokumentation\nAnimationenen, Code-Snippets und weiteren Standardelemente (Bilder, Logo, Fusszeile) sind einfach möglich resp. vorhanden\nAm unteren Bildschirmrand gibt es einen dezenten Fortschrittsbalken, sodass Referent und Zuhörer schnell wissen, wie viel Prozent der Folien / Slideanimations abgeschlossen sind bzw. noch folgen\nEin Export als PDF ist über die spezifische Ansicht im Burger-Menü und dem Druck als PDF-Datei möglich.\nNormale Quarto-Dokumente lassen sich einfach in Präsentationen umwandeln, denn die Syntax ist grundsätlich die gleiche. Man muss jedoch aufpassen, dass nicht allzuviel Inhalt auf eine Folie “gepresst” wird. Neue Folien werden mit H2-Elementen (##) generiert.\n\nAuf der Website von Quarto findet man sehr viele grundlegende sowie detaillierte Informationen, um Präsentationen mit Quarto zu erstellen."
  }
]